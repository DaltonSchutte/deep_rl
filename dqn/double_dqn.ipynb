{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import utils\n",
    "import networks\n",
    "import replay_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    \"\"\"\n",
    "        Agent to interact and learn from the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, state_size, action_size, seed, lr=0.01):\n",
    "        \"\"\"\n",
    "            ARGS:\n",
    "            state_size(int):= dimension of each state\n",
    "            action_size(int):= number of valid actions\n",
    "            seed(int):= random seed\n",
    "            lr(float):= learning rate\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.lr = lr\n",
    "        \n",
    "        #Instantiate the Policy and Target Networks\n",
    "        self.DQN_net = networks.QNetwork(self.state_size, self.action_size, seed=seed)\n",
    "        self.target_net = networks.QNetwork(self.state_size, self.action_size, seed=seed)\n",
    "        self.optimizer = optim.Adam(self.target_net.parameters(),\n",
    "                                       lr=self.lr)\n",
    "        \n",
    "        #Initialize Replay Memory and time steps\n",
    "        self.replay_mem = replay_memory.ReplayBuffer(action_size, BUFFER_SIZE, BATCH_SIZE, self.seed)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        #Add to Experience Replay Memory\n",
    "        self.replay_mem.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        self.t_step = (self.t_step+1) % UPDATE_FREQ\n",
    "        \n",
    "        #Sample from ERM if enough samples\n",
    "        if self.t_step == 0:\n",
    "            if len(self.replay_mem) > BATCH_SIZE:\n",
    "                experiences = self.replay_mem.sample()\n",
    "                self.learn(experiences, GAMMA)\n",
    "                \n",
    "    def act(self, state, epsilon=0.):\n",
    "        \"\"\"\n",
    "            Chooses an action based on the given state\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        self.DQN_net.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.DQN_net(state)\n",
    "        self.DQN_net.train()\n",
    "        \n",
    "        #Epsilon greedy selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "        \n",
    "    def learn(self, experiences, gamma):\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        #Get Q values for next state\n",
    "        Q_next_states = self.DQN_net(next_states).detach().max(1)[1].unsqueeze(1)\n",
    "        #Calculate Q targets for all actions\n",
    "        Q_targets_next = self.target_net(next_states)\n",
    "        \n",
    "        Q_targets = torch.tensor((), dtype=torch.float32).new_empty((len(next_states)))\n",
    "        \n",
    "        for i in range(0, len(next_states)):\n",
    "            terminal = dones[i]\n",
    "            \n",
    "            action = Q_next_states[i]\n",
    "            \n",
    "            if terminal:\n",
    "                Q_targets[i] = rewards[i]\n",
    "            else:\n",
    "                Q_target = rewards[i] + gamma * Q_targets_next[i][action]\n",
    "                Q_targets[i] = Q_target\n",
    "        \n",
    "        #Get expected Q values from DQN network\n",
    "        Q_expected = self.DQN_net(states).gather(1, actions)\n",
    "        \n",
    "        #Compute and minimize loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        #Update target network using policy network parameters\n",
    "        self.soft_update(self.DQN_net, self.target_net, TAU)\n",
    "        \n",
    "    def soft_update(self, online_network, target_network, tau):\n",
    "        \"\"\"\n",
    "            Soft update the target network params using policy network params according to:\n",
    "            param_target = tau*param_local + (1 - tau)*param_target\n",
    "            \n",
    "            ARGS:\n",
    "            policy_network(PyTorch Network):= network params will be copied FROM\n",
    "            target_network(PyTorch Network):= network params will be copied TO\n",
    "            tau(float):= interpolation factor\n",
    "        \"\"\"\n",
    "        for target_param, online_param in zip(target_network.parameters(),\n",
    "                                              online_network.parameters()):\n",
    "            target_param.data.copy_(tau*online_param.data + (1-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "LR = 0.1\n",
    "UPDATE_FREQ = 4\n",
    "EPSILON = 1.0\n",
    "EPS_MIN = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instantiate an agent\n",
    "agent = Agent(state_size=env.observation_space.shape[0],\n",
    "              action_size = env.action_space.n,\n",
    "              seed=0,\n",
    "              lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100 \tAverage Reward of Prev 100 Episodes: 18.444 \t Epsilon: 0.610465\n",
      "Episode: 200 \tAverage Reward of Prev 100 Episodes: 12.616 \t Epsilon: 0.374201\n",
      "Episode: 300 \tAverage Reward of Prev 100 Episodes: 10.242 \t Epsilon: 0.230899\n",
      "Episode: 400 \tAverage Reward of Prev 100 Episodes: 9.313 \t Epsilon: 0.143982\n",
      "Episode: 500 \tAverage Reward of Prev 100 Episodes: 9.162 \t Epsilon: 0.091264\n",
      "Episode: 600 \tAverage Reward of Prev 100 Episodes: 8.758 \t Epsilon: 0.059289\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-ccf52320b3ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-df9f41eddf0a>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0mexperiences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_mem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexperiences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGAMMA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-77-df9f41eddf0a>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, experiences, gamma)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmse_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQ_expected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ_targets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#Training loop\n",
    "episodes = 2000\n",
    "total_R = np.empty(episodes)\n",
    "eps = EPSILON\n",
    "max_t = 300 #Max number of iterations, gym caps this env at 200\n",
    "total_iters = 0\n",
    "\n",
    "for i_ep in range(1, episodes+1):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_r = 0\n",
    "    t = 0\n",
    "    \n",
    "    while not done and t < max_t:\n",
    "        action = agent.act(state, eps)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            reward = -200\n",
    "        \n",
    "        agent.step(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "\n",
    "        if reward == 1:\n",
    "            total_r += reward\n",
    "        \n",
    "        t += 1\n",
    "        \n",
    "        if done:\n",
    "            total_iters += t\n",
    "        \n",
    "    total_R[i_ep-1] = total_r\n",
    "    eps = EPS_MIN + (1-EPS_MIN)*np.exp(-0.005*i_ep)\n",
    "    \n",
    "    if i_ep % 100 == 0:\n",
    "        print('Episode: {} \\tAverage Reward of Prev 100 Episodes: {:.3f} \\t Epsilon: {:.6f}'.format(i_ep,\n",
    "                                                                                            total_R[i_ep-100:i_ep-1].mean(),\n",
    "                                                                                            eps))\n",
    "\n",
    "print('Average reward for last 100 episodes: ', total_R[-100:].mean())\n",
    "print('Total steps: ', total_iters)\n",
    "\n",
    "plt.plot(total_R)\n",
    "plt.title('Rewards')\n",
    "plt.show()\n",
    "\n",
    "utils.plot_running_avg(total_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
